{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torchtyping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import transformers\n",
    "from torchtyping import TensorType\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "\n",
    "#\n",
    "from arena.w2d2 import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_N_LAYERS = 12\n",
    "GPT2_N_HEADS = 8\n",
    "GPT2_VOCAB_SIZE = 50257\n",
    "GPT2_HIDDEN_SIZE = 768\n",
    "GPT2_MAX_SEQ_LEN = 1024\n",
    "GPT2_DROPOUT = 0.1\n",
    "GPT2_LN_EPS = 1e-05\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "    num_layers: int = GPT2_N_LAYERS\n",
    "    num_heads: int = GPT2_N_HEADS\n",
    "    vocab_size: int = GPT2_VOCAB_SIZE\n",
    "    hidden_size: int = GPT2_HIDDEN_SIZE\n",
    "    max_seq_len: int = GPT2_MAX_SEQ_LEN\n",
    "    dropout: float = GPT2_DROPOUT\n",
    "    layer_norm_epsilon: float = GPT2_LN_EPS\n",
    "\n",
    "config = TransformerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").train()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask(A: TensorType[..., \"seq_len\", \"seq_len\"]) -> TensorType[..., \"seq_len\", \"seq_len\"]:\n",
    "    seq_len = A.shape[-1]\n",
    "\n",
    "    mask = t.triu(t.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return A.masked_fill(mask, -np.inf)\n",
    "\n",
    "def multihead_masked_attention(\n",
    "    Q: TensorType[\"b\", \"s\", \"n*h\"], \n",
    "    K: TensorType[\"b\", \"s\", \"n*h\"], \n",
    "    num_heads: int\n",
    ") -> TensorType[\"b\", \"n\", \"s_q\", \"s_k\"]:\n",
    "    '''\n",
    "    Should return the results of multihead self-attention (after softmax, before multiplying with V)\n",
    "    '''\n",
    "    _Q = einops.rearrange(Q, \"b s (n h) -> b n s h\", n=num_heads)    \n",
    "    _K = einops.rearrange(K, \"b s (n h) -> b n s h\", n=num_heads)    \n",
    "\n",
    "    d_head = _Q.shape[-1]\n",
    "\n",
    "    A_pre = mask(\n",
    "        einsum(\"b n s_q h, b n s_k h -> b n s_q s_k\", _Q, _K)\n",
    "    ) / np.sqrt(d_head)\n",
    "\n",
    "    return t.softmax(A_pre, dim=-1)\n",
    "\n",
    "\n",
    "def multihead_masked_attention_head(\n",
    "    A: TensorType[\"b\", \"n\", \"s_q\", \"s_k\"], \n",
    "    V: TensorType[\"b\", \"s\", \"n*h\"],\n",
    "    num_heads: int\n",
    ") -> TensorType[\"batch\", \"seq\", \"n_heads*headsize\"]:\n",
    "    _V = einops.rearrange(V, \"b s (n h) -> b n s h\", n=num_heads)\n",
    "    AV: TensorType[\"b\", \"n\", \"s_q\", \"h\"] = einsum(\"b n s_q s_k, b n s_k h -> b n s_q h\", A, _V)\n",
    "    return einops.rearrange(AV, \"b n s h -> b s (n h)\") \n",
    "\n",
    "\n",
    "class GPT2Attention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "    dropout: float\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_attn = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.c_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"seq\", \"hidden_size\"]) -> TensorType[\"batch\", \"seq\", \"hidden_size\"]:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.c_attn(x).chunk(3, dim=-1)        \n",
    "        A = multihead_masked_attention(Q, K, self.num_heads)\n",
    "        A = self.attn_dropout(A)\n",
    "        h = multihead_masked_attention_head(A, V, self.num_heads)\n",
    "        x = self.c_proj(h)\n",
    "        return self.resid_dropout(x)\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, dropout: float):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_proj = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.c_fc = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.c_proj(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_fc(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, layer_norm_epsilon: float, dropout: float):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.dropout = dropout\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention(hidden_size, num_heads, dropout=dropout)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = GPT2MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.h = nn.ModuleList([\n",
    "            GPT2Block(config.hidden_size, config.num_heads, config.layer_norm_epsilon, config.dropout)\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon, elementwise_affine=True)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        pos = t.arange(x.shape[1], device=x.device)\n",
    "        x = self.wte(x) + self.wpe(pos)\n",
    "\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for h_i in self.h:\n",
    "            x = h_i(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        x = einsum(\"batch seq hidden, vocab hidden -> batch seq vocab\", x, self.wte.weight)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_fc): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (gelu): GELU(approximate=none)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_gpt = GPT2(config).train()\n",
    "my_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.0.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.1.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.2.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.3.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.4.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.5.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.6.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.7.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.8.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.9.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.10.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "h.11.attn.c_attn.weight torch.Size([2304, 768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "\n",
      "\n",
      "h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "\n",
      "\n",
      "h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "\n",
      "\n",
      "148 148\n"
     ]
    }
   ],
   "source": [
    "for (name1, tens1), (name2, tens2)  in zip(my_gpt.named_parameters(), gpt.named_parameters()):\n",
    "    s1, s2 = tens1.shape, tens2.shape\n",
    "    if s1 != s2:\n",
    "        print(name1, tens1.shape)\n",
    "        print(name2, tens2.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(len(list(my_gpt.parameters())), len(list(gpt.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1, total params = 124439808\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'num_params_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_params_1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m utils\u001b[39m.\u001b[39;49mprint_param_count(my_gpt, gpt)\n\u001b[1;32m      2\u001b[0m utils\u001b[39m.\u001b[39mtest_load_pretrained_weights(my_gpt, tokenizer)\n",
      "File \u001b[0;32m~/Projects/notes/arena/w2d2/utils.py:138\u001b[0m, in \u001b[0;36mprint_param_count\u001b[0;34m(display_df, use_state_dict, *models)\u001b[0m\n\u001b[1;32m    130\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([\n\u001b[1;32m    131\u001b[0m         {\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mname_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: name, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: \u001b[39mtuple\u001b[39m(param\u001b[39m.\u001b[39mshape), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_params_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: param\u001b[39m.\u001b[39mnumel()}\n\u001b[1;32m    132\u001b[0m         \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m iterator\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m iterator\n\u001b[1;32m    136\u001b[0m     ])\n\u001b[1;32m    137\u001b[0m     df_list\u001b[39m.\u001b[39mappend(df)\n\u001b[0;32m--> 138\u001b[0m     gmap_list\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mlog(df[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnum_params_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m]))\n\u001b[1;32m    139\u001b[0m df \u001b[39m=\u001b[39m df_list[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(df_list) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m pd\u001b[39m.\u001b[39mconcat(df_list, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(models) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/pandas/core/frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3804\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3806\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_params_1'"
     ]
    }
   ],
   "source": [
    "\n",
    "utils.print_param_count(my_gpt, gpt)\n",
    "utils.test_load_pretrained_weights(my_gpt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('notes-Szvzhu7a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e19e98c1663512e4ab445fbd0ba1d11a615d7c9869472ffe0065bf2036c088a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
