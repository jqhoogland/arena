{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import random\n","import re\n","\n","import torch as t\n","import torch.nn.functional as F\n","import transformers\n","from arena.w1.attention import DecoderOnlyTransformer, TransformerConfig\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","with open(\"./shakespeare.txt\", \"r\") as f:\n","    shakespeare = f.read()\n","    print(shakespeare[:100])    \n","\n","shakespeare_tokens = list(set(re.split(r\"\\b\", shakespeare)))\n","\n","def tokenize(text: str) -> t.Tensor:\n","    return t.tensor([shakespeare_tokens.index(token) for token in re.split(r\"\\b\", text) if token])\n","\n","def detokenize(tokens: t.Tensor) -> str:\n","    return \"\".join([shakespeare_tokens[token] for token in tokens])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","config = TransformerConfig(\n","    vocab_size=len(shakespeare_tokens),\n","    hidden_size=512,\n","    num_layers=6,\n","    num_heads=8,\n","    max_seq_len=512,\n","    dropout=0.1,\n",")\n","\n","transformer = DecoderOnlyTransformer(config)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class ShakespeareDataset(Dataset):\n","    def __init__(self, corpus: str, seq_len: int):\n","        self.inputs = []\n","        self.targets = []\n","\n","        tokens = tokenize(corpus)\n","\n","        for i in range(0, len(corpus) - seq_len, seq_len):\n","            self.inputs.append(tokens[i : i + seq_len])\n","            self.targets.append(tokens[i + 1 : i + seq_len + 1])\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx: int):\n","        input_ = self.inputs[idx]\n","        target = self.targets[idx]\n","        return input_, target\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","train_corpus = shakespeare[:int(len(shakespeare) * 0.8)]\n","test_corpus = shakespeare[int(len(shakespeare) * 0.8) :]\n","\n","train_data = ShakespeareDataset(train_corpus, 128)\n","test_data = ShakespeareDataset(test_corpus, 128)\n","\n","train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","config = TransformerConfig(\n","    vocab_size=100,\n","    hidden_size=128,\n","    num_layers=4,\n","    num_heads=4,\n","    max_seq_len=10\n",")\n","\n","transformer = DecoderOnlyTransformer(config)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","loss = t.tensor(0.)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = t.optim.Adam(transformer.parameters(), lr=1e-3)\n","\n","for epoch in range(10):\n","    for batch in train_dataloader:\n","        input_, target = batch\n","\n","        output = transformer(input_)\n","        loss = loss_fn(output.reshape(-1, 100), target.reshape(-1))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","    for batch in test_dataloader:\n","        input_, target = batch\n","\n","        output = transformer(input_)\n","        loss = loss_fn(output.reshape(-1, 100), target.reshape(-1))\n","        print(loss)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for batch in test_dataloader:\n","    input_, target = batch\n","\n","    output = transformer(input_)\n","\n","    print(output.argmax(dim=2)[0], target[0])\n","    loss = loss_fn(output.reshape(-1, 100), target.reshape(-1))\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# TODO: The model is just learning to repeat guesses. I need to play around with longer training runs & different hyperparameter combos\n"]}],"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
