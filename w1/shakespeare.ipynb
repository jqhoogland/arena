{"cells":[{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["\n","import random\n","import re\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import torch as t\n","import torch.nn.functional as F\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","import einops\n","from fancy_einsum import einsum\n","from torchtyping import TensorType\n","from tqdm.notebook import tqdm\n","from matplotlib import pyplot as plt\n","\n","device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# From previous exercises\n","\n","class PositionalEncoding(nn.Module):\n","    pe: t.Tensor\n","\n","    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n","        self.d_model = d_model\n","        self.max_len = max_len\n","        self.dropout = dropout\n","        \n","        super().__init__()\n","\n","        self.register_buffer(\n","            \"pe\",    \n","            self.encode(self.max_len, self.d_model)\n","        )\n","\n","    def encode(self, seq_len: int, embedding_dim: int) -> t.Tensor:\n","        raise NotImplementedError\n","\n","    def forward(self, x: t.Tensor) -> t.Tensor:\n","        '''\n","        x: Tensor, shape [batch, seq_len, embedding_dim]\n","        '''\n","\n","        _, seq_len, embedding_dim = x.shape\n","\n","        return x + self.pe[:seq_len, :embedding_dim].unsqueeze(0)\n","\n","\n","class SinusoidalPositionalEncoding(PositionalEncoding):\n","    def encode(self, seq_len: int, embedding_dim: int) -> t.Tensor:\n","        i = t.arange(seq_len).unsqueeze(1)\n","        d = t.arange(embedding_dim).unsqueeze(0)\n","\n","        return (\n","            t.sin(i / 10000 ** (d / embedding_dim)) * (d % 2 == 0)\n","            + t.cos(i / 10000 ** ((d - 1) / embedding_dim)) * (d % 2 == 1)\n","        )\n","\n","def mask(A: TensorType[..., \"seq_len\", \"seq_len\"]) -> TensorType[..., \"seq_len\", \"seq_len\"]:\n","    seq_len = A.shape[-1]\n","\n","    mask = t.triu(t.ones(seq_len, seq_len), diagonal=1).bool()\n","    return A.masked_fill(mask, -np.inf)\n","\n","def multihead_masked_attention(\n","    Q: TensorType[\"batch\", \"seq\", \"n_heads*headsize\"], \n","    K: TensorType[\"batch\", \"seq\", \"n_heads*headsize\"], \n","    V: TensorType[\"batch\", \"seq\", \"n_heads*headsize\"],\n","    num_heads: int\n",") -> TensorType[\"batch\", \"seq\", \"n_heads*headsize\"]:\n","    '''\n","    Should return the results of multihead self-attention.\n","\n","    Q: shape (batch, seq, n_heads*headsize)\n","    K: shape (batch, seq, n_heads*headsize)\n","    V: shape (batch, seq, n_heads*headsize)\n","    num_heads: int\n","\n","    Return: shape (batch, seq, n_heads*headsize)\n","    '''\n","    _Q = einops.rearrange(Q, \"b s (n h) -> b n s h\", n=num_heads)    \n","    _K = einops.rearrange(K, \"b s (n h) -> b n s h\", n=num_heads)    \n","    _V = einops.rearrange(V, \"b s (n h) -> b n s h\", n=num_heads)\n","\n","    d_head = _Q.shape[-1]\n","\n","    A_pre: TensorType[\"b\", \"n\", \"s_q\", \"s_k\"] = mask(\n","        einsum(\"b n s_q h, b n s_k h -> b n s_q s_k\", _Q, _K)\n","    ) / np.sqrt(d_head)\n","\n","    A: TensorType[\"b\", \"n\", \"s_q\", \"s_k\"] = t.softmax(A_pre, dim=-1)\n","    AV: TensorType[\"b\", \"n\", \"s_q\", \"h\"] = einsum(\"b n s_q s_k, b n s_k h -> b n s_q h\", A, _V)\n","\n","    return einops.rearrange(AV, \"b n s h -> b s (n h)\") \n","\n","\n","class MultiheadMaskedAttention(nn.Module):\n","    W_QKV: nn.Linear\n","    W_O: nn.Linear\n","\n","    def __init__(self, hidden_size: int, num_heads: int):\n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.head_size = hidden_size // num_heads\n","\n","        super().__init__()\n","\n","        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n","        self.W_O = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, x: TensorType[\"batch\", \"seq\", \"hidden_size\"]) -> TensorType[\"batch\", \"seq\", \"hidden_size\"]:\n","        '''\n","        x: shape (batch, seq, hidden_size)\n","\n","        Return: shape (batch, seq, hidden_size)\n","        '''\n","        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)        \n","        return self.W_O(multihead_masked_attention(Q, K, V, self.num_heads))\n","\n","\n","@dataclass(frozen=True)\n","class TransformerConfig:\n","    '''Constants used throughout your decoder-only transformer model.'''\n","\n","    num_layers: int = 6\n","    num_heads: int = 8\n","    vocab_size: int = 256\n","    hidden_size: int = 512\n","    max_seq_len: int = 512\n","    dropout: float = 0.1\n","    layer_norm_epsilon: float = 1e-05\n","\n","config = TransformerConfig()\n","\n","class MLPBlock(nn.Module):\n","\n","    def __init__(self, hidden_size: int, dropout: float):\n","        self.hidden_size = hidden_size\n","\n","        super().__init__()\n","\n","        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)\n","        self.gelu = nn.GELU()\n","        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x: t.Tensor) -> t.Tensor:\n","        x = self.linear1(x)\n","        x = self.gelu(x)\n","        x = self.linear2(x)\n","        x = self.dropout(x)\n","\n","        return x\n","\n","class DecoderBlock(nn.Module):\n","\n","    def __init__(self, hidden_size: int, num_heads: int, layer_norm_epsilon: float, dropout: float):\n","        self.hidden_size = hidden_size\n","        self.num_heads = num_heads\n","        self.layer_norm_epsilon = layer_norm_epsilon\n","        self.dropout = dropout\n","\n","        super().__init__()\n","\n","        self.attention = MultiheadMaskedAttention(hidden_size, num_heads)\n","        self.ln1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n","        self.mlp = MLPBlock(hidden_size, dropout)\n","        self.ln2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n","\n","    def forward(self, x: t.Tensor) -> t.Tensor:\n","        x = x + self.attention(self.ln1(x))\n","        x = x + self.mlp(self.ln2(x))\n","\n","        return x\n","\n","\n","class DecoderOnlyTransformer(nn.Module):\n","\n","    def __init__(self, config: TransformerConfig):\n","        self.config = config\n","\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n","        self.positional_embedding = SinusoidalPositionalEncoding(config.hidden_size, config.max_seq_len)\n","\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.decoder_blocks = nn.ModuleList([\n","            DecoderBlock(config.hidden_size, config.num_heads, config.layer_norm_epsilon, config.dropout)\n","            for _ in range(config.num_layers)\n","        ])\n","        self.ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n","        self.unembed = nn.Linear(config.hidden_size, config.vocab_size)        \n","        self.softmax = nn.Softmax(dim=2)\n","\n","    def forward(self, x: t.Tensor) -> t.Tensor:\n","        x = self.embedding(x)\n","        x = self.positional_embedding(x)\n","        x = self.dropout(x)\n","\n","        for decoder_block in self.decoder_blocks:\n","            x = decoder_block(x)\n","        \n","        x = self.ln(x)\n","        x = self.unembed(x)\n","        x = self.softmax(x)\n","\n","        return x\n","\n","\n","# %%\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['dispossess', 'inscrutable', 'cuisses', 'sonance', 'Offers', 'bequeathing', 'Accompanied', 'Ruffian', 'She', 'Cotshall'] 34543\n"]}],"source":["\n","from typing import Optional, Union\n","\n","\n","with open(\"./shakespeare.txt\", \"r\") as f:\n","    shakespeare = f.read()\n","\n","class WordsTokenizer():\n","    def __init__(self, corpus: str):\n","        self.from_tokens = list(set(re.split(r\"\\b\", corpus)))\n","        self.to_tokens = {k: i for i, k in enumerate(self.from_tokens)}\n","        self.vocab_size = len(self.from_tokens)\n","\n","    def encode(self, initial_text: str, return_tensors: Optional[str] = None) -> Union[list, np.ndarray, t.Tensor]:\n","        tensors_list = [self.to_tokens[s] for s in re.split(r\"\\b\", initial_text) if len(s) > 0]\n","        if return_tensors is None:\n","            return tensors_list\n","        elif return_tensors == \"pt\":\n","            return t.tensor(tensors_list)\n","        elif return_tensors == \"np\":\n","            return np.array(tensors_list)\n","        else:\n","            raise Exception(\"Unexpected value for `return_tensors`.\")\n","\n","    def decode(self, list_of_ids: Union[t.Tensor, list[int]]) -> str:\n","        return ''.join([self.from_tokens[token] for token in list_of_ids])\n","\n","tokenizer = WordsTokenizer(shakespeare)\n","shakespeare_encoded = tokenizer.encode(shakespeare, return_tensors=\"pt\")\n","print(tokenizer.from_tokens[:10], len(tokenizer.from_tokens))"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["\n","class ShakespeareDataset(Dataset):\n","    def __init__(self, tokens: t.Tensor | list[int], seq_len: int):\n","        self.inputs = []\n","        self.targets = []\n","\n","        for i in range(0, len(tokens) - seq_len - 1, seq_len):\n","            self.inputs.append(tokens[i : i + seq_len])\n","            self.targets.append(tokens[i + 1 : i + seq_len + 1])\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, idx: int):\n","        input_ = self.inputs[idx]\n","        target = self.targets[idx]\n","        return input_, target\n","\n","train_corpus = shakespeare_encoded[:int(len(shakespeare_encoded) * 0.8)]\n","test_corpus = shakespeare_encoded[int(len(shakespeare_encoded) * 0.8) :]\n","\n","train_data = ShakespeareDataset(train_corpus, 128) # type: ignore\n","test_data = ShakespeareDataset(test_corpus, 128) # type: ignore\n","\n","train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["\n","config = TransformerConfig(\n","    vocab_size=tokenizer.vocab_size,\n","    hidden_size=128,\n","    num_layers=4,\n","    num_heads=4,\n","    max_seq_len=128\n",")\n","\n","transformer = DecoderOnlyTransformer(config)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["\n","def train(model, optimizer, loss_fn, trainloader, epochs, dataset_name=None, plot=True):\n","\n","    loss_list = []\n","\n","    for epoch in range(epochs):\n","        \n","        progress_bar = tqdm(trainloader)\n","        \n","        for (x, y) in progress_bar:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            optimizer.zero_grad()\n","            \n","            logits = model(x)\n","            # logits dimensions are (batch, seq, digits), but we care about probabilities for each digit\n","            # so we need to reshape into (batch * seq, digits)\n","            loss = loss_fn(einops.rearrange(logits, \"b s d -> (b s) d\"), y.flatten())\n","            loss.backward()\n","\n","            optimizer.step()\n","            \n","            progress_bar.set_description(f\"epoch = {epoch+1}, loss = {loss.item():.4f}\")\n","\n","            loss_list.append(loss.item())\n","    \n","        if plot:\n","            plt.plot(loss_list)\n","            plt.title(f\"{dataset_name} loss\")\n","            plt.show()\n","    \n","    return model    "]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"452ecfcefdf34c93b219233f4425a5de","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/195 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [74], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(transformer\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 4\u001b[0m train(transformer, optimizer, loss_fn, train_dataloader, \u001b[39m10\u001b[39;49m)\n","Cell \u001b[0;32mIn [73], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, trainloader, epochs, dataset_name, plot)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# logits dimensions are (batch, seq, digits), but we care about probabilities for each digit\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# so we need to reshape into (batch * seq, digits)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(einops\u001b[39m.\u001b[39mrearrange(logits, \u001b[39m\"\u001b[39m\u001b[39mb s d -> (b s) d\u001b[39m\u001b[39m\"\u001b[39m), y\u001b[39m.\u001b[39mflatten())\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m progress_bar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch = \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[0;32m~/.local/share/virtualenvs/notes-Szvzhu7a/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["optimizer = t.optim.Adam(transformer.parameters(), lr=1e-3)\n","loss_fn = nn.CrossEntropyLoss()\n","\n","train(transformer, optimizer, loss_fn, train_dataloader, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.8 ('notes-Szvzhu7a')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e19e98c1663512e4ab445fbd0ba1d11a615d7c9869472ffe0065bf2036c088a"}}},"nbformat":4,"nbformat_minor":2}
