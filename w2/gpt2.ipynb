{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torchtyping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'utils' from 'arena.w2' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39meinops\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39marena\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mw2\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'utils' from 'arena.w2' (unknown location)"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import transformers\n",
    "from torchtyping import TensorType\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "\n",
    "#\n",
    "from arena.w2 import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_VOCAB_SIZE = 50257\n",
    "GPT2_HIDDEN_SIZE = 768\n",
    "GPT2_MAX_SEQ_LEN = 1024\n",
    "GPT2_DROPOUT = 0.1\n",
    "GPT2_LN_EPS = 1e-05\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "    num_layers: int = 6\n",
    "    num_heads: int = 8\n",
    "    vocab_size: int = GPT2_VOCAB_SIZE\n",
    "    hidden_size: int = GPT2_HIDDEN_SIZE\n",
    "    max_seq_len: int = GPT2_MAX_SEQ_LEN\n",
    "    dropout: float = GPT2_DROPOUT\n",
    "    layer_norm_epsilon: float = GPT2_LN_EPS\n",
    "\n",
    "config = TransformerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1997931988.py, line 162)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [5], line 162\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mask(A: TensorType[..., \"seq_len\", \"seq_len\"]) -> TensorType[..., \"seq_len\", \"seq_len\"]:\n",
    "    seq_len = A.shape[-1]\n",
    "\n",
    "    mask = t.triu(t.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return A.masked_fill(mask, -np.inf)\n",
    "\n",
    "def multihead_masked_attention(\n",
    "    Q: TensorType[\"b\", \"s\", \"n*h\"], \n",
    "    K: TensorType[\"b\", \"s\", \"n*h\"], \n",
    "    num_heads: int\n",
    ") -> TensorType[\"b\", \"n\", \"s_q\", \"s_k\"]:\n",
    "    '''\n",
    "    Should return the results of multihead self-attention (after softmax, before multiplying with V)\n",
    "    '''\n",
    "    _Q = einops.rearrange(Q, \"b s (n h) -> b n s h\", n=num_heads)    \n",
    "    _K = einops.rearrange(K, \"b s (n h) -> b n s h\", n=num_heads)    \n",
    "\n",
    "    d_head = _Q.shape[-1]\n",
    "\n",
    "    A_pre = mask(\n",
    "        einsum(\"b n s_q h, b n s_k h -> b n s_q s_k\", _Q, _K)\n",
    "    ) / np.sqrt(d_head)\n",
    "\n",
    "    return t.softmax(A_pre, dim=-1)\n",
    "\n",
    "\n",
    "def multihead_masked_attention_head(\n",
    "    A: TensorType[\"b\", \"n\", \"s_q\", \"s_k\"], \n",
    "    V: TensorType[\"b\", \"s\", \"n*h\"],\n",
    "    num_heads: int\n",
    ") -> TensorType[\"batch\", \"seq\", \"n_heads*headsize\"]:\n",
    "    _V = einops.rearrange(V, \"b s (n h) -> b n s h\", n=num_heads)\n",
    "    AV: TensorType[\"b\", \"n\", \"s_q\", \"h\"] = einsum(\"b n s_q s_k, b n s_k h -> b n s_q h\", A, _V)\n",
    "    return einops.rearrange(AV, \"b n s h -> b s (n h)\") \n",
    "\n",
    "\n",
    "class GPT2Attention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "    dropout: float\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attn_droppout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: TensorType[\"batch\", \"seq\", \"hidden_size\"]) -> TensorType[\"batch\", \"seq\", \"hidden_size\"]:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)        \n",
    "        A = multihead_masked_attention(Q, K, self.num_heads)\n",
    "        A = self.attn_droppout(A)\n",
    "        h = multihead_masked_attention_head(A, V, self.num_heads)\n",
    "        x = self.W_O(h)\n",
    "        return self.resid_dropout(x)\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, dropout: float):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, layer_norm_epsilon: float, dropout: float):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.dropout = dropout\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.attn = GPT2Attention(hidden_size, num_heads, dropout=dropout)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_size, eps=layer_norm_epsilon)\n",
    "        self.mlp = GPT2MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        self.config = config\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.wpe = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.h = nn.ModuleList([\n",
    "            GPT2Block(config.hidden_size, config.num_heads, config.layer_norm_epsilon, config.dropout)\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon, elementwise_affine=True)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)        \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        pos = t.arange(x.shape[1], device=x.device)\n",
    "        x = self.token_embedding(x) + self.positional_embedding(pos)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x)\n",
    "        \n",
    "        x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.test_load_pretrained_weights(my_gpt, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('notes-Szvzhu7a')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e19e98c1663512e4ab445fbd0ba1d11a615d7c9869472ffe0065bf2036c088a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
